\chapter{Literature Review}
\label{cha:litreview}

The term Network Tomography first came into use in \cite{vardi_network_1996}, focusing on the estimation of network traffic intensity from individual link data. This estimation is accomplished using a known routing table and frequent measurements of link traffic under an assumed (Poisson) distribution. These techniques are lifted directly from related work examining statistical properties of Positron Emission Tomography in the medical field (Used in PET scans) (\cite{vardi_statistical_1985}). This allows for the internal state of a network to be reconstructed using only periphery measurements and as such is extremely useful in cases where monitoring of network’s where there is no access to internal nodes is required, i.e. subsections of the internet or large GPON’s such as the National Broadband Network (NBN) (\cite{gregory_how_2019}).\par
Early work in the field such as Vardi’s work also touches on the notion of deterministic or Markovian routing, specifically under the assumption that fixed routing can be considered as a special case of deterministic routing and as such they both can be solved given the same set of predefined assumptions hold true. The problem of tomography was originally presented as a ‘LININPOS’ or LINear INverse POSitive problem and only explored as a purely statistical problem; using a more practical lens we would now term this as a form of passive tomography.\par
Quickly following this initial statistical paper, the study of the emerging field of network tomography (referred to here on wards as simply tomography) split into two related but distinct approaches, that of statisticians/mathematicians and that of computer scientists. Computer scientists further decomposed tomography into passive and active tomography. Passive tomography uses measurements aggregated from observations of all network traffic to calculate properties such as traffic intensity (\cite{vardi_network_1996}, \cite{coates_network_2001}) and network topology (\cite{hailiang_network_2009}, \cite{zhang_topology_2014}, \cite{hailiang_network_2009}). The determination of source-destination traffic intensity is often referred to as traffic matrix or origin destination tomography and focuses on aggregation of internal link measurements to determine network wide traffic (\cite{cao_time-varying_2000}). In contrast, network topology identification focuses on forming a best effort approximation of all node-to-node connections based on observed distance metrics. As each of these approaches require access to all network traffic they require a set of edge routers from which this can be observed. Such a set is not always obtainable in real world situations due to hurdles such as traffic anonymization preventing capture of packet sources and the large proportion of edge routers required to be controlled by an observer limiting scalability (\cite{leung_measurement-based_2002}). Active tomography on the other hand tends to be more applicable to real world systems as it requires less assumed knowledge of the network's state and the observers access to components of the network.\par
Active tomography, commonly referred to within literature as either Quality of Service (QoS) tomography or network performance tomography (\cite{lawrence_network_2006}, \cite{barnes_stochastic_2020}), can be subdivided into two main approaches of boolean and additive tomography. Each of these approaches is characterised by its representation of internal performance, additionally each approach results in different performance metrics being exposed for evaluation. Boolean tomography represents the internal state of links as an unknown boolean variable, in the conventional case of failure localization this variable corresponds to a “normal” and “failed”' state, as such the inverse problem arises from the success/failure of a source destination path being the logical OR of it’s composite paths (\cite{duffield_network_2006}). In contrast, additive tomography represents the internal state of links as an unknown non-negative values to model non-binary performance metrics - primarily link delay. Given this representation in additive tomography the inverse problem arises from the total delay on a path being the sum of all delays of it’s composite paths. Although boolean and additive tomography have many differences in goals and methodologies there are some similarities between them, specifically in the conditions sufficient to allow for inferences to be drawn. \par
The topological conditions under which inferences are able to be made in both additive and boolean tomography vary depending on routing policy used for monitor probes. An excellent derivation of these conditions can be found in \cite{he_network_2021} however here we summarise and present these conditions under different routing mechanisms in §2.6. In all cases however, for both additive and link tomography end to end measures are used to infer individual link level performances characteristics of the network. All work up to this point has focused on the notion of deterministic link performance, however in real world networks this assumption is seldom correct.\par
The investigation of stochastic performance on router links, and consequently routing behaviours, has become increasingly popular in recent years. The use of tomography to draw link level performance inferences in such an environment is termed stochastic network tomography and most current work on the topic, like that of active tomography, can be split into two subcategories of boolean and non-boolean. Both these techniques seek to identify the distribution of associated metrics according to an unknown parameter $\theta$, the goal of tomography is therefore to estimate $\theta$ end to end observations (\cite{he_fisher_2015}). Boolean stochastic tomography focuses on the packet loss of a network where packet drops are non-deterministically dropped from the network, potentially by an intermittent node or link failure. In contrast, non-boolean stochastic tomography looks to infer information around expected delays on each link, possibly caused by signal congestion or signal handling techniques such as deep packet inspection (DPI) (\cite{el-maghraby_survey_2017}). Both of these tasks are accomplished through the use of Fisher Information Matrices (FIM) with known and controllable routes to develop Maximum Likelihood Estimators (MLEs); the specifics of this technique are covered in detail in \cref{sec:Bparameterestimation}.\par
\textbf{Add \cite{kolar_distributed_2020} here re distributed implementation of nefarious node identification in stochastic networks.}
The primary goal of our work seeks to apply these techniques of packet delay estimation to improve efficacy and efficiency of this nefarious router identification.\par
Finally, the work of \cite{barnes_stochastic_2020} the use of tomography to infer link level performance in a stochastic system was foregone in favour of the ability to identify ‘nefarious’ nodes. This was accomplished through the development of an MCMC model to simulate expected queue lengths at nodes given possible configurations of nefarious routers. Nefarious routers in this work are represented as ones which have a probability at any discrete time step of not forwarding a packet to the next router. The delay distributions generated by the MCMC model are then compared to that observed at monitor nodes and the most closely correlated is shown to be a correct prediction of the set of nefarious routers. 
\todo{Probe path allocation}
\todo{Optimum experimental designs}
