\chapter{Literature Review}
\label{cha:litreview}

The term network tomography first came into use in \cite{vardi_network_1996}, focusing on the estimation of network traffic intensity from individual link data. This estimation is accomplished using a known routing table and frequent measurements of link traffic under an assumed (Poisson) distribution. These techniques are lifted directly from related work examining statistical properties of Positron Emission Tomography in the medical field (Used in PET scans) (\cite{vardi_statistical_1985}). This allows for the internal state of a network to be reconstructed using only peripheral measurements and as such is extremely useful in cases of network monitoring where no access to internal nodes is required; eg. in subsections of the internet or large GPONs such as the National Broadband Network (NBN) (\cite{gregory_how_2019}).\par
Early work in the field (such as Vardi’s) also touches on the notion of deterministic or Markovian routing. Specifically, under the assumption that fixed routing can be considered a special case of deterministic routing and, as such, that both can be solved given the same set of predefined assumptions. The problem of tomography was originally presented as a ‘LININPOS’ or LINear INverse POSitive problem, and only explored as a purely statistical matter. Using a more practical lens, we would now term this as a form of passive tomography.\par
Shortly after this initial statistical paper, the study of the emerging field of network tomography (referred to hereafter simply as tomography) split into two related but distinct approaches; that of statisticians/mathematicians, and that of computer scientists.\par
Computer scientists further decomposed tomography into passive and active tomography. Passive tomography uses measurements aggregated from observations of all network traffic so as to calculate properties such as traffic intensity (\cite{vardi_network_1996}, \cite{coates_network_2001}) and network topology (\cite{hailiang_network_2009}, \cite{zhang_topology_2014}, \cite{hailiang_network_2009}).\par
The determination of source-destination traffic intensity is often referred to as traffic matrix or origin destination tomography. This branch of network tomography focuses on aggregation of internal link measurements to determine network wide traffic intensity (\cite{cao_time-varying_2000}). In contrast, network topology identification focuses on forming a best-effort approximation of all node-to-node connections based on observed distance metrics.\par
As each of these approaches require access to all network traffic, they require a set of edge routers from which this traffic can be observed. However, such a set is not always obtainable in real-world situations due to hurdles such as traffic anonymization (preventing capture of packet sources) and the large proportion of edge routers required to be controlled by an observer (limiting scalability) (\cite{leung_measurement-based_2002}). In contrast, active tomography is more applicable to real-world systems as it requires less assumed knowledge of the network's state and also less access to components of the network.\par
Active tomography, commonly referred to within literature as either Quality of Service (QoS) tomography or network performance tomography (\cite{lawrence_network_2006}, \cite{barnes_stochastic_2020}), can be subdivided into two main approaches of boolean and additive tomography. Each of these approaches is characterised by its representation of internal performance. Additionally, each approach results in different performance metrics being exposed for evaluation.\par
Boolean tomography represents the internal state of links as an unknown boolean variable. In the conventional case of failure localisation, this variable corresponds to a “normal” and “failed” state. Therefore, the inverse problem arises from the success/failure of a source destination path being the logical OR of its composite paths (\cite{duffield_network_2006}). In contrast, additive tomography represents the internal state of links as an unknown non-negative value so as to model non-binary performance metrics (primarily link delay). Given this representation in additive tomography, the inverse problem arises from the total delay on a path being the sum of all delays of its composite paths. Although boolean and additive tomography have many differences in goals and methodologies, there are some similarities between them. Specifically, in the conditions sufficient to allow for inferences to be drawn. \par
The topological conditions under which inferences are able to be made in both additive and boolean tomography vary depending on which routing policy is used for monitor probes. A derivation of these conditions is given in \cite{he_network_2021}. We summarise and present these conditions under different routing mechanisms in \cref{sec:Broutingmechanisms}. Regardless of routing conditions, in both additive and link tomography, end-to-end measures are used to infer individual link level performance characteristics of the network. All work up to this point has focused on the notion of deterministic link performance. However, in real world networks this assumption is seldom correct.\par
The investigation of stochastic performance on router links, and consequently on routing behaviours, has become increasingly popular in recent years. The use of tomography to draw link level performance inferences in such an environment is termed stochastic network tomography. Most current work on the topic, like that of active tomography, can be split into the two subcategories of boolean and non-boolean. Both these techniques seek to identify the distribution of associated metrics according to an unknown parameter $\theta$. The goal of tomography is therefore to estimate $\theta$ from end-to-end observations (\cite{he_fisher_2015}).\par
Boolean stochastic tomography focuses on the packet loss of a network where packets are non-deterministically dropped from the network (potentially due to an intermittent node or link failure). In contrast, non-boolean stochastic tomography looks to infer information around expected delays on each link (possibly caused by signal congestion or signal handling techniques such as deep packet inspection (\cite{el-maghraby_survey_2017})). Both these tasks are accomplished by using Fisher information matrices (FIM) with known and controllable routes to develop Maximum Likelihood Estimators (MLEs). The specifics of this technique are covered in detail in \cref{sec:Bparameterestimation}.\par
Alongside development of new network tomography techniques, research has been undertaken to optimise existing applications of network tomography. The most notable work surrounding optimisation has been around the selection of which set paths between monitors to probe. This work is notable as optimisation to selection of probe paths can reduce the traffic overhead required to generate accurate results using network tomography. This problem has been approached from three distinct directions in the works of \cite{zheng_minimizing_2013}, \cite{tootaghaj_parsimonious_2018}, and \cite{rahali_unicast_2019}.\par
In their 2013 study of link performance monitoring techniques, \cite{zheng_minimizing_2013}, developed an algorithm to select a minimal set of path between monitors which still allow routers to be identified using network tomography. In contrast, the Greedy-Min-Cost-Rank (GMCR) algorithm was developed in \cite{tootaghaj_parsimonious_2018} to select a set of paths between monitors with the smallest possible number of links across all paths. Most recently, the k-paths method (an extension of the work of \cite{lawrence_network_2006}) and the Evolutionary Sampling algorithm (ESA) were developed in \cite{rahali_unicast_2019}, with the goal of selecting the minimal set of paths between monitors.\par
We note that ESA, developed as an extension to k-paths, dominated k-paths in both computational efficiency and accuracy. This was due to their use of a technique based on random sampling designed to avoid the high computation and memory complexity. Additionally, both of these solutions accounted for the costs associated with the collation of metrics from all monitors at a single location post-analysis.\par
Finally, in the work of \cite{barnes_stochastic_2020} the use of tomography to infer link level performance in a stochastic system was foregone in favour of the ability to identify nefarious nodes. This was accomplished through the development of a mathematical model to simulate expected queue lengths at nodes, given all possible configurations of nefarious routers. Nefarious routers in this work are represented as having a probability, at any discrete time step, of not forwarding a packet to another router. The delay distributions generated by the MCMC model are then compared to that observed at monitor nodes. The most closely correlated is shown to be a correct prediction of the set of nefarious routers. The primary goal of our work is to apply techniques of packet delay estimation so as to improve the efficacy and efficiency of this nefarious router identification.\par